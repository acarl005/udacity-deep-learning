{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (187473, 28, 28) (187473,)\n",
      "Validation set (9867, 28, 28) (9867,)\n",
      "Test set (9788, 28, 28) (9788,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (187473, 784) (187473, 10)\n",
      "Validation set (9867, 784) (9867, 10)\n",
      "Test set (9788, 784) (9788, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute the logits for the given inputs and the layers\n",
    "def calc_logits(dataset, *layers, dropouts=False):\n",
    "    # variable for the current layer\n",
    "    activations = dataset\n",
    "    # apply each layer to the previous outputs\n",
    "    for i, layer in enumerate(layers):\n",
    "        activations = tf.matmul(activations, layer[0]) + layer[1]\n",
    "        # run a ReLU in between each layer, but not on the final layer\n",
    "        if i < len(layers) - 1:\n",
    "            activations = tf.nn.relu(activations)\n",
    "            if dropouts:\n",
    "                activations = tf.nn.dropout(activations, 0.5)\n",
    "    return activations\n",
    "\n",
    "# define tf variables for the NN layers given the sizes of the layers\n",
    "def construct_layers(*sizes):\n",
    "    layers = []\n",
    "    for i in range(1, len(sizes)):\n",
    "        # initialize weights. stddev must be chosen carefully to avoid neuron saturation\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [sizes[i - 1], sizes[i]],\n",
    "                stddev=np.sqrt(2.0 / sizes[i - 1])))\n",
    "        biases = tf.Variable(tf.zeros([sizes[i]]))\n",
    "        layers.append([weights, biases])\n",
    "    return layers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "learning_rate = 0.5\n",
    "\n",
    "regress_graph = tf.Graph()\n",
    "with regress_graph.as_default():\n",
    "    # add the data to the tensorflow graph\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=(batch_size, image_size ** 2))\n",
    "    tf_train_labels = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    regul_coeff = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # declare the weights and biases\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size ** 2, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # run the math and calculate the error\n",
    "    logits = calc_logits(tf_train_dataset, [weights, biases])\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    # add regularization term to the error\n",
    "    loss += regul_coeff * tf.nn.l2_loss(weights)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(calc_logits(tf_valid_dataset, [weights, biases]))\n",
    "    test_prediction = tf.nn.softmax(calc_logits(tf_test_dataset, [weights, biases]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 46s, sys: 21.5 s, total: 2min 7s\n",
      "Wall time: 1min 25s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEOCAYAAACEiBAqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f957053da20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_regress(num_steps, regul_coeff_val, verbose=False):\n",
    "    with tf.Session(graph=regress_graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "            # calculate the starting index for the next batch\n",
    "            offset = (step * batch_size) % (len(train_labels) - batch_size)\n",
    "            # get next batch for this iteration of stochastic gradient descent\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            data = {\n",
    "                tf_train_dataset: batch_data,\n",
    "                tf_train_labels: batch_labels,\n",
    "                regul_coeff: regul_coeff_val }\n",
    "            \n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction],\n",
    "                feed_dict=data)\n",
    "            \n",
    "            # report on the progress\n",
    "            if verbose and step % (num_steps // 10) == 0:\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        \n",
    "        return accuracy(test_prediction.eval(), test_labels)\n",
    "\n",
    "# test several different regulariztion terms\n",
    "regul_coeffs = np.power(10, np.arange(-4, -1.7, 0.2), dtype=np.float32)\n",
    "%time accuracy_vals = [run_regress(3001, regul_coeff) for regul_coeff in regul_coeffs]\n",
    "plt.semilogx(regul_coeffs, accuracy_vals)\n",
    "plt.title('Test accuracy vs regularization coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal regularization parameter:\n",
      "0.001\n",
      "Minibatch loss at step 0: 24.333015\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 12.2%\n",
      "Minibatch loss at step 300: 3.542153\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 75.1%\n",
      "Minibatch loss at step 600: 2.869265\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 900: 2.005164\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1200: 1.657538\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 1500: 1.028831\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 1800: 1.211218\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2100: 1.100792\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2400: 0.776256\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 2700: 0.822515\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 3000: 1.015545\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 82.4%\n",
      "CPU times: user 8.71 s, sys: 1.91 s, total: 10.6 s\n",
      "Wall time: 6.69 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88.782182263996731"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Optimal regularization parameter:')\n",
    "optimal_regul = regul_coeffs[np.argmax(accuracy_vals)]\n",
    "print(optimal_regul)\n",
    "\n",
    "%time run_regress(3001,  optimal_regul, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "learning_rate = 0.5\n",
    "\n",
    "nn_graph = tf.Graph()\n",
    "with nn_graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=(batch_size, image_size ** 2))\n",
    "    tf_train_labels = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    regul_coeff = tf.placeholder(tf.float32)\n",
    "    \n",
    "\n",
    "    layers = construct_layers(image_size ** 2, num_hidden_nodes, num_labels)\n",
    "    \n",
    "    logits = calc_logits(tf_train_dataset, *layers)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    loss += regul_coeff * (tf.nn.l2_loss(layers[0][0]) + tf.nn.l2_loss(layers[1][0]))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(calc_logits(tf_valid_dataset, *layers))\n",
    "    test_prediction = tf.nn.softmax(calc_logits(tf_test_dataset, *layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_nn(num_steps, regul_coeff_val, verbose=False):\n",
    "    with tf.Session(graph=nn_graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (len(train_labels) - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            data = {\n",
    "                tf_train_dataset: batch_data,\n",
    "                tf_train_labels: batch_labels,\n",
    "                regul_coeff: regul_coeff_val }\n",
    "        \n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction],\n",
    "                feed_dict=data)\n",
    "            \n",
    "            if verbose and step % (num_steps // 10) == 0:\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "                \n",
    "        return accuracy(test_prediction.eval(), test_labels)\n",
    "  \n",
    "\n",
    "regul_coeffs = np.power(10, np.arange(-4.8, -2.0, 0.2), dtype=np.float32)\n",
    "%time accuracy_vals = [run_nn(1001, regul_coeff) for regul_coeff in regul_coeffs]\n",
    "plt.semilogx(regul_coeffs, accuracy_vals)\n",
    "plt.title('Test accuracy vs regularization coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal regularization parameter:\n",
      "0.000398107\n",
      "Minibatch loss at step 0: 2.722213\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 42.4%\n",
      "Minibatch loss at step 100: 0.784347\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 200: 0.887314\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 300: 0.824271\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 400: 0.814958\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 500: 0.701326\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 600: 0.877330\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 700: 0.717140\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 800: 0.635151\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 900: 0.730562\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 1000: 0.611371\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.7%\n",
      "CPU times: user 8.24 s, sys: 1.98 s, total: 10.2 s\n",
      "Wall time: 13 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92.817736003269303"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Optimal regularization parameter:')\n",
    "optimal_regul = regul_coeffs[np.argmax(accuracy_vals)]\n",
    "print(optimal_regul)\n",
    "\n",
    "%time run_nn(1001,  optimal_regul, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 2.367433\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 37.9%\n",
      "Minibatch loss at step 50: 0.015061\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.6%\n",
      "Minibatch loss at step 100: 0.006539\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.7%\n",
      "Minibatch loss at step 150: 0.004131\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 200: 0.002866\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 250: 0.002226\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 300: 0.001842\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 350: 0.001503\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 400: 0.001297\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.9%\n",
      "Minibatch loss at step 450: 0.001157\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.9%\n",
      "Minibatch loss at step 500: 0.001001\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.9%\n",
      "CPU times: user 4.88 s, sys: 1.08 s, total: 5.96 s\n",
      "Wall time: 7.11 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76.726604004903962"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_overfit(num_steps, regul_coeff_val, num_batches, verbose=False):\n",
    "    with tf.Session(graph=nn_graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "            offset = step % num_batches\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            data = {\n",
    "                tf_train_dataset: batch_data,\n",
    "                tf_train_labels: batch_labels,\n",
    "                regul_coeff: regul_coeff_val }\n",
    "        \n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction],\n",
    "                feed_dict=data)\n",
    "            \n",
    "            if verbose and step % (num_steps // 10) == 0:\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "                \n",
    "        return accuracy(test_prediction.eval(), test_labels)\n",
    "  \n",
    "\n",
    "%time run_overfit(501, 0, 3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "learning_rate = 0.5\n",
    "\n",
    "dropouts_graph = tf.Graph()\n",
    "with dropouts_graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=(batch_size, image_size ** 2))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    layers = construct_layers(image_size ** 2, num_hidden_nodes, num_labels)\n",
    "    \n",
    "    logits = calc_logits(tf_train_dataset, *layers, dropouts=True)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(calc_logits(tf_valid_dataset, *layers))\n",
    "    test_prediction = tf.nn.softmax(calc_logits(tf_test_dataset, *layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 2.529994\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 38.1%\n",
      "Minibatch loss at step 300: 0.651115\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 600: 0.672698\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 900: 0.549887\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 1200: 0.608670\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 1500: 0.358303\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 1800: 0.526905\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2100: 0.646354\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 2400: 0.441141\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 2700: 0.572631\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 3000: 0.651024\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 88.0%\n",
      "CPU times: user 20.4 s, sys: 4.34 s, total: 24.7 s\n",
      "Wall time: 30.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93.46138128320392"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_knockout_nn(num_steps, verbose=False):\n",
    "    with tf.Session(graph=dropouts_graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (len(train_labels) - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            data = {\n",
    "                tf_train_dataset: batch_data,\n",
    "                tf_train_labels: batch_labels }\n",
    "        \n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction],\n",
    "                feed_dict=data)\n",
    "            \n",
    "            if verbose and step % (num_steps // 10) == 0:\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "                \n",
    "        return accuracy(test_prediction.eval(), test_labels)\n",
    "  \n",
    "\n",
    "%time run_knockout_nn(3001, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "num_hidden_nodes3 = 256\n",
    "\n",
    "deep_graph = tf.Graph()\n",
    "with deep_graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=(batch_size, image_size ** 2))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    layers = construct_layers(\n",
    "        image_size ** 2,\n",
    "        num_hidden_nodes1,\n",
    "        num_hidden_nodes2,\n",
    "        num_hidden_nodes3,\n",
    "        num_labels)\n",
    "\n",
    "    logits = calc_logits(tf_train_dataset, *layers)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(calc_logits(tf_valid_dataset, *layers))\n",
    "    test_prediction = tf.nn.softmax(calc_logits(tf_test_dataset, *layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 2.327041\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 33.7%\n",
      "Minibatch loss at step 500: 0.342179\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 1000: 0.322500\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 1500: 0.212072\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 2000: 0.205471\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 2500: 0.234393\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 3000: 0.427662\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 3500: 0.192685\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 4000: 0.198824\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 4500: 0.219865\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 5000: 0.223745\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.8%\n",
      "CPU times: user 48.9 s, sys: 10 s, total: 58.9 s\n",
      "Wall time: 1min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95.647731916632608"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_deep(num_steps):\n",
    "    with tf.Session(graph=deep_graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (len(train_labels) - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            data = {\n",
    "                tf_train_dataset: batch_data,\n",
    "                tf_train_labels: batch_labels }\n",
    "        \n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=data)\n",
    "        \n",
    "            if step % (num_steps // 10) == 0:\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "                \n",
    "        return accuracy(test_prediction.eval(), test_labels)\n",
    "    \n",
    "%time run_deep(5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
